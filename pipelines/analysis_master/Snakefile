import yaml 
from pytools.persistent_dict import PersistentDict

##### Configuration #####

config["output_path"] = config["output_path"].rstrip("/")
config["annotated_path"] = config["annotated_path"].rstrip("/")
config["basecalled_path"] = config["basecalled_path"].rstrip("/")

barcodes = ""
for i in config["barcodes"].split(','):
    barcodes+=" {}".format(i)

sample_name = config["sample_name"] if config.get("sample_name") else "_".join(config["barcodes"].split(','))

stems = PersistentDict("stem_store")

##### Master workflow #####

rule all:
    input:
        config["output_path"] + f"/consensus_sequences/{sample_name}.fasta"

rule binlorry:
    input:
    params:
        path_to_reads = config["basecalled_path"],
        report_dir = config["annotated_path"],
        outdir = config["output_path"],
        min_read_length = config["min_read_length"],
        max_read_length = config["max_read_length"],
        barcodes = barcodes,
        sample_name = sample_name,
        output_prefix= config["output_path"] + "/binned_{sample}"
    output:
        fastq=config["output_path"] + "/binned_{sample}.fastq",
        csv=config["output_path"] + "/binned_{sample}.csv"
    shell:
        "binlorry -i {params.path_to_reads:q} "
        "-t {params.report_dir:q} "
        "-o {params.output_prefix:q} "
        "-n {params.min_read_length} "
        "-x {params.max_read_length} "
        "-v 0 "
        "--filter-by barcode {params.barcodes} "
        "--out-report"



# rule assign_amplicon:
#     input:
#         csv= rules.binlorry.output.csv,
#         amplicons = config["amplicon_csv"]
#     params:
#         sample = "{sample}",
#         path_to_script = workflow.current_basedir
#     output:
#         config["output_path"] + "/binned_{sample}.amp_info.csv"
#     shell:
#         """
#         python {params.path_to_script}/assign_amplicon.py \
#         --csv {input.csv} \
#         --out_csv {output} \
#         --amplicons {input.amplicons}
#         """

#bin-by reference at this point (RSV A or B)

# rule assess_sample:
#     input:
#         reads= rules.binlorry.output.fastq,
#         csv= rules.binlorry.output.csv,
#         refs = config["references_file"]
#     params:
#         sample = "{sample}",
#         output_path = config["output_path"] + "/binned_{sample}",
#         min_reads = config["min_reads"],
#         min_pcent = config["min_pcent"],
#         bin_factor = config["bin_factor"],
#         path_to_script = workflow.current_basedir
#     output:
#         t = temp(config["output_path"] + "/binned_{sample}/temp.txt")
#     run:

#         for i in shell(
#             "python {params.path_to_script}/parse_rsv_ref_and_depth.py "
#             "--reads {input.reads} "
#             "--csv {input.csv} "
#             "--output_path {params.output_path} "
#             "--bin_factor {params.bin_factor} "
#             "--min_reads {params.min_reads} "
#             "--min_pcent {params.min_pcent} "
#             "--references {input.refs} "
#             "--sample {params.sample}  && touch {output.t}", iterable=True):

#             stems.store("analysis_stem",i)

#need the exact refererence to run the samples through

rule process_sample:
    input:
        fastq=config["output_path"] + "/binned_{sample}.fastq",
        csv=config["output_path"] + "/binned_{sample}.csv",
        config=workflow.current_basedir+"/config.yaml"
    params:
        sample = "{sample}",
        output_path= config["output_path"],
        path = workflow.current_basedir
    output:
        cns = config["output_path"] + "/consensus_sequences/{sample}.fasta"
    shell:
        """
        snakemake --nolock --snakefile {params.path}/../process_sample/Snakefile \
        --configfile {input.config} \
        --config \
        analysis_stem={params.sample} \
        output_path={params.output_path} \
        sample={params.sample}
        """
